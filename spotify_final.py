# -*- coding: utf-8 -*-
"""Spotify_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1W5MIa90aN-ysTGYvEwTUzIqaTZb50Hg4

# Import libraries
"""

import pandas as pd
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import GridSearchCV
import pickle

"""# Data Wrangling

## Data Collection
"""

data = pd.read_csv('../../Spotify_final/spotify-2023.csv', encoding='ISO-8859-1')
data.head()

# Listing the columns of the Dataframe
columns = data.columns.tolist()
print(columns)

"""## Data Cleaning

### Missing values
"""

missing_values = data.isnull().sum()
print(missing_values)

data.shape

"""### Dealing with Missing values

in_shazam_charts:

Analysis: Since this represents a ranking on the Shazam charts, missing values likely indicate that the song didn't achieve a rank.

Our Strategy: We opted to handle the missing data in two ways:

For interpretability, we created a copy of the column and filled missing values with a "Not Ranked" placeholder. This maintains clarity for anyone inspecting the dataset manually.

For machine learning readiness, another copy of the column was made. Here, missing values are replaced with a numerical value (max_rank + 1). This ensures models receive purely numerical input without losing the information that these songs weren't ranked.
This dual approach retains the integrity of the original data while making it usable for various analytical purposes.
"""

data['in_shazam_charts']

# 1 Not ranked(Human readable)
data['in_shazam_charts_readable'] = data['in_shazam_charts']
data['in_shazam_charts'] = data['in_shazam_charts'].replace({',':''}, regex=True).astype(float)

data['in_shazam_charts']

# 2 max_rank+1(Ml processing)
data['in_shazam_charts_forMl'] = data['in_shazam_charts'].copy()
max_rank = data['in_shazam_charts_forMl'].max()

data['in_shazam_charts_readable'].fillna('Not Ranked', inplace=True)
data['in_shazam_charts_forMl'].fillna(max_rank+1,inplace=True)

data.drop('in_shazam_charts',axis=1, inplace=True)

data['in_shazam_charts_forMl'] = data['in_shazam_charts_forMl'].astype(int)

"""key:

Analysis: This attribute signifies the musical key in which the song is composed. Assigning mean, median, or mode might be inappropriate and potentially misleading unless the missing values occur randomly without a discernable pattern.

Our Strategy: Given the nature of the 'key' attribute and considering its importance in our analysis, we have decided to remove the rows with missing key values to maintain the integrity and quality of our dataset. This approach ensures that we're working with complete data for each song in our subsequent analyses.
"""

data = data.dropna(subset = ['key'])

data.shape

"""### Dealing with data types"""

data.dtypes

# Combine year, month, day in single datetime col
data['release_date'] = pd.to_datetime(data['released_year'].astype(str)+'-'+data['released_month'].astype(str)+'-'+data['released_day'].astype(str), errors='coerce')
data['release_date'].head()

# Combine streams, in_dreezer_playlist to appropiate numerical or categorical data type
# coerce means replacing alphabetic value by nan
data['streams'] = pd.to_numeric(data['streams'], errors='coerce')
data['in_deezer_playlists'] = pd.to_numeric(data['in_deezer_playlists'], errors='coerce')

data['streams'].isna().sum()

data.shape

data = data.dropna(subset=['streams'])

data['in_deezer_playlists'].isna().sum()

data['in_deezer_playlists'].fillna(0,inplace=True)

# Creating encoded col for mode and key
mode_encoded = pd.get_dummies(data['mode'], prefix='mode')
data = pd.concat([data,mode_encoded], axis=1)

label_encoder = LabelEncoder()
data['key_encoded'] = label_encoder.fit_transform(data['key'].astype(str))

print(data[['mode', *mode_encoded.columns, 'key', 'key_encoded']].head())

# Percentage columns
per_col = ['danceability_%', 'valence_%', 'energy_%', 'acousticness_%',
       'instrumentalness_%', 'liveness_%', 'speechiness_%']

for col in per_col:
    print(f"{col}: min= {data[col].min()}, max = {data[col].max()}")

# New scaled percentage for ML algo
for col in per_col:
    data[col+'_forML'] = data[col]/100.0

data.dtypes

"""# Data Analysis

## Summary
"""

summary_statistics = data.describe()
summary_statistics

"""## Histogram"""

sns.set_style('whitegrid')

data.select_dtypes(include = ['float64', 'int64']).hist(bins=30, figsize=(20,20), color='green', edgecolor='black')
plt.tight_layout()
plt.show()

"""## Correlation Matrix"""

numeric_data = data.select_dtypes(include=[np.number])

correlation_matrix = numeric_data.corr()
plt.figure(figsize=(15,15))
sns.heatmap(correlation_matrix, annot=True, cmap='rainbow')
plt.title('Correalation matrix')
plt.show()

"""# Predictions (Random Forest)

## With Default hyperparameters
"""

cols_to_use = ['artist_count', 'released_year', 'released_month', 'released_day',
    'in_spotify_playlists', 'in_spotify_charts', 'in_apple_playlists', 'in_apple_charts',
    'in_deezer_playlists', 'in_deezer_charts', 'bpm', 'danceability_%_forML',
    'valence_%_forML', 'energy_%_forML', 'acousticness_%_forML', 'instrumentalness_%_forML',
    'liveness_%_forML', 'speechiness_%_forML', 'key_encoded']

X = data[cols_to_use]
y = data['streams']

X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5,random_state=42)

rf = RandomForestRegressor()
rf.fit(X_train, y_train)

# Prediction on validation set
y_val_pred = rf.predict(X_val)

mse = mean_squared_error(y_val, y_val_pred)
mae = mean_absolute_error(y_val, y_val_pred)
r2 = r2_score(y_val, y_val_pred)

print("Validation Set evaluation")
print(f"MSE: {mse}")
print(f"MAE: {mae}")
print(f"R2: {r2}")

# Prediction on test set
y_test_pred = rf.predict(X_test)

mse = mean_squared_error(y_test, y_test_pred)
mae = mean_absolute_error(y_test, y_test_pred)
r2 = r2_score(y_test, y_test_pred)

print("Test Set evaluation")
print(f"MSE: {mse}")
print(f"MAE: {mae}")
print(f"R2: {r2}")

"""### Important features"""

feature_importances = rf.feature_importances_
features_df = pd.DataFrame({
    'Feature': cols_to_use,
    'Importance': feature_importances
})
print('Feature Importances')
print(features_df.sort_values(by='Importance', ascending=False))

"""## With Best Parameters"""

param_grid = {
    'n_estimators': [50,100,200],
    'max_features': ['auto','sqrt'],
    'max_depth': [None,10,20,30],
    'min_samples_split': [2,5,10],
    'min_samples_leaf': [1,2,4]
}

grid_search = GridSearchCV(estimator=RandomForestRegressor(), param_grid=param_grid
                           ,cv=3, n_jobs=-1, verbose=2, scoring='r2')

grid_search.fit(X_train, y_train)
print('Best Parameters:', grid_search.best_params_)

y_val_pred = grid_search.best_estimator_.predict(X_val)

mse = mean_squared_error(y_val, y_val_pred)
mae = mean_absolute_error(y_val, y_val_pred)
r2 = r2_score(y_val, y_val_pred)
print(f"Mean Squared Error (MSE): {mse}")
print(f"Mean Absolute Error (MAE): {mae}")
print(f"R^2 Score: {r2}")

"""### Evaluation"""

y_test_pred = grid_search.best_estimator_.predict(X_test)

mse_test = mean_squared_error(y_test, y_test_pred)
mae_test = mean_absolute_error(y_test, y_test_pred)
r2_test = r2_score(y_test, y_test_pred)

print(f"Test Set Evaluation:")
print(f"Mean Squared Error (MSE): {mse_test}")
print(f"Mean Absolute Error (MAE): {mae_test}")
print(f"R^2 Score: {r2_test}")

"""### Feature Importance"""

importances = grid_search.best_estimator_.feature_importances_
features = cols_to_use

df = pd.DataFrame({'Feature': features,
                   'Importance': importances})
df = df.sort_values(by='Importance', ascending=False)

plt.figure(figsize=(12,8))
sns.barplot(x='Importance', y='Feature', data=df)
plt.title('Feature Importance')
plt.show()

"""### Actual vs Predicted Streams"""

plt.figure(figsize=(12,8))
sns.scatterplot(x=y_test, y=y_test_pred, alpha=0.6, edgecolor=None)
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)],
         color='red',linestyle='--', lw=2)
plt.xlabel('Actusl Streams', fontsize=14)
plt.ylabel('Predicted Streams', fontsize=14)
plt.title('Actal vs Predicted', fontsize=16)
plt.grid(True, which='both', linestyle='--', linewidth=0.5)
plt.tight_layout()
plt.show()

pickle.dump(grid_search, open("../../Spotify_final/model.pkl", 'wb'))

pickle.dump(grid_search, open("../../Spotify_final/model.sav", 'wb'))